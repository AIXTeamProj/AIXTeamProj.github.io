<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI 프로젝트 문서</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        h1 {
            font-size: 2.5rem;
            color: #2c3e50;
            text-align: center;
            margin: 2rem 0;
            font-weight: 600;
            letter-spacing: -0.5px;
        }

        h2 {
            font-size: 1.8rem;
            color: #2c3e50;
            margin: 1.5rem 0 1rem 0;
            font-weight: 500;
            letter-spacing: -0.3px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 0.5rem;
        }

        h3 {
            font-size: 1.4rem;
            color: #34495e;
            margin: 1.2rem 0 0.8rem 0;
            font-weight: 500;
        }

        h4 {
            font-size: 1.2rem;
            color: #34495e;
            margin: 1rem 0 0.6rem 0;
            font-weight: 500;
        }

        h5 {
            font-size: 1.1rem;
            color: #34495e;
            margin: 0.8rem 0 0.5rem 0;
            font-weight: 500;
        }

        .section {
            margin-bottom: 2rem;
            padding: 1rem;
            background: #ffffff;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .section:last-child {
            border-bottom: none;
        }

        .member-info {
            margin-bottom: 20px;
        }

        #disqus_thread {
            margin-top: 20px;
            padding: 20px;
            background-color: #f9f9f9;
            border-radius: 5px;
        }

        .methodology-content, .section-content {
            padding: 1.5rem;
            background: #f8f9fa;
            border-radius: 8px;
            margin-bottom: 2rem;
        }

        .methodology-content h4 {
            color: #2c3e50;
            margin-top: 25px;
            margin-bottom: 15px;
        }

        .methodology-content p {
            margin-bottom: 15px;
            line-height: 1.8;
        }

        .methodology-content ul, .methodology-content ol {
            margin-left: 20px;
            margin-bottom: 15px;
        }

        .methodology-content li {
            margin-bottom: 8px;
            line-height: 1.6;
        }

        .process-summary {
            background-color: #fff;
            padding: 15px;
            border-radius: 5px;
            border-left: 4px solid #2c3e50;
        }

        .process-summary ol {
            margin: 0;
            padding-left: 20px;
        }

        .process-summary li {
            margin-bottom: 10px;
        }

        .model-figure {
            margin: 2rem 0;
            text-align: center;
        }

        .model-architecture {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .figure-caption {
            margin-top: 1rem;
            color: #7f8c8d;
            font-size: 0.9rem;
        }

        .goals-list {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin-top: 1.5rem;
        }

        .goal-item {
            background: #ffffff;
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        .goal-item h5 {
            color: #2c3e50;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .goal-item p {
            margin: 0;
            line-height: 1.6;
        }

        .quote-block {
            margin: 2rem 0;
            padding: 1.5rem;
            background: #f8f9fa;
            border-left: 4px solid #3498db;
            border-radius: 0 8px 8px 0;
        }

        .quote-block blockquote {
            margin: 0;
            font-style: italic;
            color: #2c3e50;
        }

        .quote-block footer {
            margin-top: 1rem;
            color: #7f8c8d;
            font-size: 0.9rem;
        }

        .video-container {
            margin: 30px 0;
            text-align: center;
        }

        .comparison-video {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .workflow-container {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .algorithm-interaction {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            overflow-x: auto;
        }

        .workflow-diagram {
            font-family: monospace;
            white-space: pre;
            margin: 0;
            color: #333;
        }

        .section-content h5 {
            color: #2c3e50;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .section-content ul {
            margin-left: 20px;
            margin-bottom: 15px;
        }

        .section-content ul ul {
            margin-top: 5px;
            margin-bottom: 5px;
        }

        .comparison-container {
            display: flex;
            gap: 20px;
            margin: 20px 0;
        }

        .comparison-block {
            flex: 1;
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .performance-comparison,
        .scalability-analysis,
        .practicality-analysis {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .hybrid-analysis {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
        }

        .hybrid-analysis p {
            margin-bottom: 15px;
        }

        .section-content a {
            color: #2c3e50;
            text-decoration: none;
            border-bottom: 1px dotted #2c3e50;
        }

        .section-content a:hover {
            color: #3498db;
            border-bottom: 1px solid #3498db;
        }

        .keyword-definitions {
            margin: 20px 0;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }

        .keyword-definition {
            margin-bottom: 20px;
            padding: 15px;
            background-color: white;
            border-radius: 6px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .keyword-definition h5 {
            color: #2c3e50;
            margin-bottom: 10px;
        }

        .keyword-definition p {
            color: #34495e;
            line-height: 1.6;
        }

        a {
            color: #2c3e50;
            text-decoration: none;
            border-bottom: 1px dotted #3498db;
            transition: all 0.3s ease;
        }

        a:hover {
            color: #3498db;
            border-bottom: 1px solid #3498db;
        }

        .principle-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 1.5rem;
            margin: 1.5rem 0;
        }

        .principle-item {
            background: #ffffff;
            padding: 1.5rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        .principle-item h5 {
            color: #2c3e50;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .math-formula {
            margin: 2rem 0;
            padding: 1rem;
            background: #f8f9fa;
            border-radius: 8px;
            text-align: center;
        }

        .math-formula p {
            margin: 0.5rem 0;
        }

        .math-formula .MathJax {
            font-size: 1.2em;
        }

        .concept-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .concept-item {
            background: #ffffff;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .concept-item h3 {
            color: #2c3e50;
            margin-bottom: 1rem;
            font-size: 1.5rem;
        }

        .concept-item h4 {
            color: #34495e;
            margin: 1.5rem 0 0.5rem 0;
            font-size: 1.2rem;
        }

        .concept-item ul {
            list-style-type: disc;
            margin-left: 1.5rem;
            margin-bottom: 1rem;
        }

        .concept-item li {
            margin: 0.5rem 0;
            line-height: 1.6;
        }

        .concept-item strong {
            color: #3498db;
        }

        .members-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 2rem;
            padding: 2rem 0;
        }

        .member-card {
            background: linear-gradient(145deg, #ffffff, #f5f5f5);
            border-radius: 15px;
            padding: 2rem;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .member-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 6px 20px rgba(0, 0, 0, 0.15);
        }

        .member-info h3 {
            color: #2c3e50;
            font-size: 1.5rem;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }

        .member-role {
            color: #3498db;
            font-size: 1.1rem;
            font-weight: 500;
            margin-bottom: 0.5rem;
        }

        .member-detail {
            color: #7f8c8d;
            font-size: 1rem;
            line-height: 1.5;
        }

        @media (max-width: 768px) {
            .members-grid {
                grid-template-columns: 1fr;
                gap: 1.5rem;
            }
        }
    </style>
</head>
<body>
    <h1>Diffusion-based Text Generation LLM</h1>
    
    <div class="section">
        <h2>Members</h2>
        <div class="section-content">
            <p><strong>김도현</strong> (파이낸스경영학과) - earth02052203@gmail.com</p>
            <p><strong>이시웅</strong> (정보시스템학과) - bluewings02@hanyang.ac.kr</p>
        </div>
    </div>

    <div class="section">
        <h2>Presentation Video</h2>
        <div class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/1UKTODjtqLw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
    </div>

    <div class="section">
        <h2>I. Proposal</h2>
        <div class="methodology-content">
            <h3>1. Motivation</h3>
            <h4>1.1 기존 모델의 한계</h4>
            <p>
                2025년 현재, 이미지 생성 분야는 <a href="#diffusion-model">Stable Diffusion Model</a>의 지배적인 영향력 하에 있습니다. 
                텍스트 프롬프트를 통해 고품질의 이미지를 생성하는 이 모델의 성능은, 
                유사한 방식으로 언어 정보의 생성이 가능할 것이라는 아이디어의 시발점이 되었습니다.
            </p>
            <p>
                그러나 언어는 이미지와 달리 순차적인 특성을 지닙니다. 
                즉, 단어들이 특정 순서로 배열되어 의미를 형성하므로, 
                이러한 언어의 고유한 특성을 <a href="#diffusion">Diffusion</a> 방식이 효과적으로 반영할 수 있는지에 대한 
                근본적인 의문이 제기되었습니다. 
                기존의 <a href="#transformer">Transformer</a> 기반 모델들이 언어의 순차성을 다루는 데 최적화되어 있음을 고려할 때, 
                <a href="#diffusion">Diffusion</a> 방식의 텍스트 생성 적용 가능성은 심층적인 검토를 필요로 했습니다.
            </p>

            <h4>1.2 탐구의 전환점: 구글 I/O 2025</h4>
            <p>
                본 연구의 방향성은 2025년 5월 20일에 개최된 구글 I/O를 통해 확고해졌습니다. 
                해당 행사에서 <a href="#diffusion-llm">Diffusion-Based LLM Model</a>에 대한 기업들의 활발한 연구 및 개발 동향이 확인되었습니다. 
                이는 <a href="#diffusion">Diffusion</a> 방식이 텍스트 생성 분야에서도 실제적인 적용 가능성을 지니고 있음을 시사하며, 
                단순한 아이디어를 넘어선 기술적 타당성을 입증하는 계기가 되었습니다.
            </p>
            <p>
                이러한 정보는 기존 <a href="#llm">LLM</a>의 한계를 보완하고 새로운 응용 분야를 개척할 수 있는 
                <a href="#diffusion-llm">Diffusion LLM</a>의 잠재력을 확인하는 중요한 전환점이 되었습니다. 
                이미지 생성 분야의 혁신이 텍스트 생성 분야에서도 재현될 수 있다는 기대감은 
                본 주제를 심층적으로 탐구하게 된 주요 원동력입니다.
            </p>

            <h3>2. Goals</h3>
            <h4>2.1 탐구 목표: <a href="#diffusion-llm">Diffusion LLM</a>의 이해 및 응용 방안 모색</h4>
            <p>
                상기 배경을 바탕으로, 본 문서는 <a href="#diffusion-llm">Diffusion LLM</a>에 대한 다각적인 탐구를 목표로 합니다. 
                구체적인 연구 목표는 다음과 같습니다:
            </p>
            <div class="goals-list">
                <div class="goal-item">
                    <h5>정보 획득 및 분석</h5>
                    <p><a href="#diffusion-llm">Diffusion LLM</a>의 핵심 원리, 최신 연구 동향, 그리고 기존 <a href="#llm">LLM</a>과의 기술적 차이점을 심층적으로 분석하여 포괄적인 이해를 구축합니다.</p>
                </div>
                <div class="goal-item">
                    <h5>응용 방안 모색</h5>
                    <p><a href="#diffusion-llm">Diffusion LLM</a>이 단순한 텍스트 생성을 넘어, 특정 스타일의 문체 생성, 창의적인 스토리텔링, 코드 생성 등 다양한 혁신적인 응용 분야에 적용될 수 있는 가능성을 탐색합니다.</p>
                </div>
                <div class="goal-item">
                    <h5>기술적 함의 도출</h5>
                    <p>기존 <a href="#llm">LLM</a>과의 비교 분석을 통해 <a href="#diffusion-llm">Diffusion LLM</a>의 독자적인 강점과 한계를 파악하고, 향후 연구 및 개발 방향에 대한 기술적 함의를 도출합니다.</p>
                </div>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>II. Methodology</h2>
        <div class="section-content">
            <h3>1. Pre-Knowledge: Stable Diffusion Model</h3>
            <div class="methodology-content">
                <div class="model-figure">
                    <img src="imgs/figureofldm.png" alt="Stable Diffusion Model Architecture" class="model-architecture">
                    <p class="figure-caption">그림 1. LDM의 전체 구조</p>
                </div>

                <h4>1.1 Diffusion Process (Forward Process)</h4>
                <p>
                    실제 이미지에 점진적으로 노이즈를 추가하여 완전한 무작위 노이즈로 변환하는 과정입니다.<br>
                    이 과정을 통해 모델은 각 노이즈 단계별 데이터 분포를 학습하게 됩니다.
                </p>

                <h4>1.2 Denoising Process (Reverse Process)</h4>
                <p>
                    노이즈가 추가된 이미지를 원래 상태로 복원하는 과정입니다.<br>
                    이 과정은 UNet이라는 모델에 의해 수행되며, UNet은 다음과 같은 특징을 가집니다:
                </p>
                <ul>
                    <li>여러 해상도에서 특징을 추출하고 조합하는 CNN 구조</li>
                    <li>노이즈가 추가된 이미지, 시간 단계, 텍스트 인코딩 정보를 입력으로 받아 노이즈 성분을 예측</li>
                </ul>

                <h4>1.3 Latent Diffusion (잠재 확산)</h4>
                <p>
                    고해상도 이미지(예: 512×512) 공간에서의 직접적인 학습은 계산량이 매우 많습니다.<br>
                    이를 해결하기 위해 다음과 같은 과정을 거칩니다:
                </p>
                <ol>
                    <li>VAE(Variational Autoencoder)를 사용하여 원본 이미지를 잠재 공간으로 압축</li>
                    <li>잠재 공간에서 diffusion 과정 수행</li>
                    <li>디코더를 통해 원본 이미지 공간으로 복원</li>
                </ol>
                <p>이러한 방식으로 메모리 사용량을 줄이고 처리 속도를 향상시킬 수 있습니다.</p>

                <h4>1.4 텍스트 조건부 생성 (Text Conditioning)</h4>
                <p>
                    CLIP Text Encoder를 활용하여 입력 텍스트를 벡터로 인코딩합니다.<br>
                    이 텍스트 인코딩은 UNet에 조건으로 제공되어, 텍스트의 의미를 반영한 이미지를 생성하게 됩니다.<br>
                    이 과정은 cross-attention 메커니즘을 통해 구현됩니다.
                </p>

                <h4>1.5 전체 프로세스 요약</h4>
                <div class="process-summary">
                    <ol>
                        <li>입력 텍스트 → CLIP text encoder → 텍스트 임베딩</li>
                        <li>랜덤 노이즈 → UNet에 주입 (시간 정보 + 텍스트 임베딩과 함께)</li>
                        <li>노이즈를 점진적으로 제거 → latent representation 복원</li>
                        <li>latent representation → VAE decoder → 최종 이미지</li>
                    </ol>
                </div>
            </div>

            <h3>2. Pre-Knowledge: Diffusion LLM</h3>
            <div class="methodology-content">
                <h4>2.1 텍스트 생성의 새로운 패러다임</h4>
                <div class="quote-block">
                    <blockquote>
                        <p>
                            Traditional <a href="#autoregressive">autoregressive</a> language models generate text one word – or token – at a time. 
                            This sequential process can be slow, and limit the quality and coherence of the output.
                        </p>
                        <p class="translation">
                            전통적인 <a href="#autoregressive">자동회귀</a> 언어 모델은 한 번에 하나의 단어나 토큰을 생성합니다. 
                            이러한 순차적 과정은 느릴 수 있으며, 출력의 품질과 일관성을 제한할 수 있습니다.
                        </p>
                        <p>
                            <a href="#diffusion">Diffusion</a> models work differently. Instead of predicting text directly, they learn to generate outputs 
                            by refining noise, step-by-step. This means they can iterate on a solution very quickly and error correct 
                            during the generation process. This helps them excel at tasks like editing, including in the context of math and code.
                        </p>
                        <p class="translation">
                            <a href="#diffusion">디퓨전</a> 모델은 다르게 작동합니다. 텍스트를 직접 예측하는 대신, 
                            노이즈를 단계적으로 개선하여 출력을 생성하는 방법을 학습합니다. 
                            이는 매우 빠르게 해결책을 반복하고 생성 과정에서 오류를 수정할 수 있다는 것을 의미합니다. 
                            이는 수학과 코드를 포함한 편집 작업에서 특히 뛰어난 성능을 보여줍니다.
                        </p>
                        <footer>- Google DeepMind</footer>
                    </blockquote>
                </div>

                <h4>2.2 <a href="#diffusion-llm">디퓨전 LLM</a>의 작동 원리</h4>
                <div class="principle-container">
                    <div class="principle-item">
                        <h5>2.2.1 <a href="#masking">마스킹</a> 기반 생성</h5>
                        <p>
                            <a href="#diffusion-llm">디퓨전 LLM</a>은 텍스트의 일부를 <a href="#mask">[MASK]</a> 토큰으로 대체하고, 
                            모델이 이 <a href="#mask">마스크</a>된 부분을 예측하는 방식으로 작동합니다. 
                            이 과정은 점진적인 <a href="#denoising">디노이징</a>을 통해 이루어지며, 
                            각 단계마다 더 정확한 예측을 수행합니다.
                        </p>
                    </div>
                    <div class="principle-item">
                        <h5>2.2.2 <a href="#parallel">병렬</a> 처리</h5>
                        <p>
                            기존 <a href="#autoregressive">자동회귀</a> 모델과 달리, <a href="#diffusion-llm">디퓨전 LLM</a>은 
                            여러 토큰을 동시에 생성할 수 있습니다. 이는 블록 단위 <a href="#parallel">병렬 처리</a>를 통해 
                            구현되며, 생성 속도를 크게 향상시킵니다.
                        </p>
                    </div>
                    <div class="principle-item">
                        <h5>2.2.3 <a href="#sampling">샘플링</a> 전략</h5>
                        <p>
                            <a href="#diffusion-llm">디퓨전 LLM</a>은 <a href="#classifier-free-guidance">Classifier-free Guidance</a>를 적용하여 
                            생성 품질을 향상시키고, 신뢰도 기반 토큰 선택을 통해 더 정확한 출력을 생성합니다.
                        </p>
                    </div>
                </div>



                <h4>2.3 성능 비교</h4>
                <p>
                    <a href="#mercury-coder">MercuryCoder</a>와 Gemini Flash 2.0 Lite</a>의 벤치마크 비교 결과, 
                    <a href="#diffusion-llm">디퓨전 LLM</a>이 기존 <a href="#autoregressive">자동회귀</a> <a href="#llm">LLM</a>에 비해 
                    훨씬 빠른 생성 속도를 보여줍니다 (32k tokens MAX).
                </p>
                <div class="model-figure">
                    <img src="imgs/gdlmbm1.jpg" alt="Benchmark Comparison with Gemini Flash 2.0 Lite" class="model-architecture">
                    <p class="figure-caption">그림 1. <a href="#mercury-coder">MercuryCoder</a>와 Gemini Flash 2.0 Lite의 벤치마크 비교</p>
                </div>

                <div class="model-figure">
                    <video controls class="model-architecture">
                        <source src="imgs/mercurycoderspeed.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="figure-caption">그림 2. <a href="#mercury-coder">MercuryCoder</a>와 일반 <a href="#llm">LLM</a>의 코드 생성 속도 비교</p>
                </div>
            </div>

            <h3>3. Algorithms</h3>
            <div class="section-content">
                <h4>3.1 LLaDA 소개</h4>
                <p>
                    <a href="#llada">LLaDA</a>(Large Language Diffusion with mAsking)는 8B 스케일의 <a href="#diffusion-model">디퓨전 모델</a>로, 처음부터 완전히 학습되었으며 <a href="#llama3">LLaMA3 8B</a>와 비슷한 성능을 보여주는 혁신적인 모델입니다. 이 모델은 <a href="#autoregressive">자동회귀적 메커니즘</a> 자체가 아닌, 생성 모델링의 핵심 원리인 최대 우도 추정을 통해 실제 언어 분포를 근사하는 방식으로 <a href="#llm">LLM</a>의 지능을 구현합니다.
                </p>
                <p>
                    <a href="#mercury-coder">MercuryCoder</a>나 <a href="#gemini-diffusion">Gemini Diffusion</a>과 같은 API 기반 모델들과 달리, <a href="#llada">LLaDA</a>는 오픈 소스로 공개되어 있어 로컬 환경에서 실행하며 알고리즘과 코드 구조를 직접 분석할 수 있습니다. 본 연구에서는 <a href="#llada">LLaDA</a>의 알고리즘을 상세히 분석하여 <a href="#diffusion-llm">디퓨전 기반 LLM</a>의 작동 방식을 파악했습니다.
                </p>

                <h4>3.2 LLaDA의 핵심 특징</h4>
                <ul>
                    <li><a href="#masking">마스킹</a> 기반 <a href="#diffusion-model">디퓨전 모델</a></li>
                    <li>표준 <a href="#pretraining">사전학습</a>과 <a href="#sft">SFT</a>(Supervised Fine-Tuning) 과정</li>
                    <li><a href="#diffusion">디퓨전</a> 기반 <a href="#sampling">샘플링</a> 방식</li>
                    <li><a href="#pretraining">사전학습</a> 시 모든 토큰을 무작위로 <a href="#masking">마스킹</a> (비율 α)</li>
                    <li><a href="#sft">SFT</a> 시에는 응답 토큰만 <a href="#masking">마스킹</a></li>
                    <li>전체 <a href="#masking">마스킹</a>(α=1)에서 언마스킹(α=0)까지의 <a href="#diffusion">디퓨전</a> 시뮬레이션</li>
                    <li>각 단계에서 모든 <a href="#mask">마스크</a>를 동시에 예측</li>
                    <li>유연한 <a href="#remasking">리마스킹</a> 기능</li>
                </ul>

                <h4>3.3 확장성</h4>
                <p>
                    <a href="#llada">LLaDA</a>는 인상적인 확장성을 보여주며, 동일한 데이터에 대한 <a href="#autoregressive">자동회귀</a> 기반 모델과 경쟁력 있는 성능을 보여줍니다. 이는 <a href="#diffusion">디퓨전</a> 기반 접근 방식이 대규모 언어 모델링에서도 효과적일 수 있음을 입증합니다.
                </p>

                <h4>3.4 LLaDA의 핵심 알고리즘</h4>
                <p>
                    <a href="#llada">LLaDA</a>는 여러 혁신적인 알고리즘을 조합하여 효율적인 텍스트 생성을 구현합니다.
                </p>

                <h5>3.4.1 <a href="#diffusion-algorithm">디퓨전 알고리즘</a> (Diffusion Algorithm)</h5>
                <ul>
                    <li>목적: 노이즈에서 의미 있는 텍스트로의 변환</li>
                    <li>작동 방식:
                        <ul>
                            <li>초기 상태: 모든 위치가 <a href="#mask">[MASK]</a> 토큰</li>
                            <li>점진적 <a href="#denoising">디노이징</a>: <a href="#mask">마스크</a>를 순차적으로 제거</li>
                            <li>조건부 확률: p(x_t-1 | x_t) 계산</li>
                        </ul>
                    </li>
                </ul>

                <h5>3.4.2 <a href="#gumbel-max">Gumbel-Max/Softmax 알고리즘</a></h5>
                <p>
                    <a href="#diffusion-llm">디퓨전 LLM</a>에서 토큰 선택을 위해 사용되는 두 가지 관련된 알고리즘입니다:
                </p>

                <h6>Gumbel-Max</h6>
                <ul>
                    <li>이산 확률 분포에서 샘플링을 수행하는 기본 알고리즘</li>
                    <li>수식: \(\text{Gumbel-Max}(x) = \arg\max_i (x_i + g_i)\)</li>
                    <li>여기서 \(g_i\)는 Gumbel 분포에서 샘플링된 노이즈</li>
                    <li>미분 불가능한 argmax 연산을 사용하여 정확한 이산 샘플링 수행</li>
                </ul>

                <h6>Gumbel-Softmax</h6>
                <ul>
                    <li>Gumbel-Max의 미분 가능한 근사 버전</li>
                    <li>수식: \(\text{Gumbel-Softmax}(x, \tau) = \text{softmax}\left(\frac{x + g}{\tau}\right)\)</li>
                    <li>Temperature 파라미터 \(\tau\)를 통해 샘플링의 다양성 조절
                        <ul>
                            <li>\(\tau \to 0\): 결정적인(deterministic) 샘플링</li>
                            <li>\(\tau \to \infty\): 균일한(uniform) 샘플링</li>
                        </ul>
                    </li>
                    <li>학습 과정에서 사용되며, 미분 가능성을 통해 역전파 가능</li>
                </ul>

                <div class="model-figure">
                    <img src="imgs/gumbelsoftmaxcpmax.png" alt="Gumbel-Max vs Gumbel-Softmax Comparison" class="model-architecture">
                    <p class="figure-caption">그림 3. Gumbel-Max와 Gumbel-Softmax 알고리즘의 결과 비교</p>
                </div>

                <div class="model-figure">
                    <img src="imgs/gumbelbytemp.png" alt="Gumbel-Softmax behavior with different temperatures" class="model-architecture">
                    <p class="figure-caption">그림 4. Temperature에 따른 Gumbel-Softmax 알고리즘의 동작 변화</p>
                </div>

                <h5>3.4.3 <a href="#classifier-free-guidance">Classifier-free Guidance 알고리즘</a></h5>
                <ul>
                    <li>목적: 조건부 생성 품질 향상</li>
                    <li>작동: 조건부/무조건부 생성의 가중 평균</li>
                </ul>

                <h5>3.4.4 <a href="#block-parallel">블록 단위 반자동회귀 알고리즘</a></h5>
                <ul>
                    <li>목적: <a href="#parallel">병렬 처리</a> 효율화</li>
                    <li>작동: 텍스트를 블록으로 분할하여 <a href="#parallel">병렬 처리</a></li>
                </ul>

                <h5>3.4.5 <a href="#confidence-based">신뢰도 기반 토큰 선택 알고리즘</a></h5>
                <ul>
                    <li>목적: 생성 품질 향상</li>
                    <li>작동: 낮은 신뢰도 토큰 <a href="#remasking">재마스킹</a></li>
                </ul>

                <h4>3.5 전체 워크플로우</h4>
                <div class="workflow-container">
                    <div class="model-figure">
                        <img src="imgs/diff_normal_150ms.gif" alt="LLaDA Text Generation Process" class="model-architecture">
                        <p class="figure-caption">그림 5. <a href="#llada">LLaDA</a>의 텍스트 생성 과정 시각화</p>
                    </div>

                    <h5>3.5.1 <a href="#initialization">초기화 단계</a></h5>
                    <ul>
                        <li>입력 텍스트 <a href="#tokenization">토큰화</a></li>
                        <li>응답 부분을 <a href="#mask">[MASK]</a> 토큰으로 초기화</li>
                        <li>제약 조건 파싱 및 적용</li>
                    </ul>

                    <h5>3.5.2 <a href="#block-processing">블록 처리 단계</a></h5>
                    <ul>
                        <li>텍스트를 block_length 크기의 블록으로 분할</li>
                        <li>각 블록에 대해:
                            <ul>
                                <li><a href="#mask">마스크</a> 인덱스 계산</li>
                                <li>토큰 전이 수 계산</li>
                            </ul>
                        </li>
                    </ul>

                    <h5>3.5.3 <a href="#denoising-step">디노이징 단계</a></h5>
                    <ul>
                        <li>각 스텝마다:
                            <ul>
                                <li><a href="#classifier-free-guidance">Classifier-free Guidance</a> 적용</li>
                                <li><a href="#gumbel-max">Gumbel-Max</a> <a href="#sampling">샘플링</a></li>
                                <li>신뢰도 계산</li>
                                <li>토큰 선택 및 적용</li>
                                <li>시각화 상태 업데이트</li>
                            </ul>
                        </li>
                    </ul>

                    <h5>3.5.4 <a href="#post-processing">후처리 단계</a></h5>
                    <ul>
                        <li>최종 텍스트 디코딩</li>
                        <li>시각화 결과 정리</li>
                    </ul>
                </div>

                <h4>3.6 알고리즘 간 상호작용</h4>
                <div class="algorithm-interaction">
                    <pre class="workflow-diagram">
[입력 텍스트]
    ↓
[<a href="#tokenization">토큰화</a>]
    ↓
[<a href="#initialization">초기화</a>: <a href="#mask">MASK</a> 토큰]
    ↓
[<a href="#block-processing">블록 분할</a>]
    ↓
[<a href="#denoising-step">디노이징 루프</a>]
    ├→ [<a href="#classifier-free-guidance">Classifier-free Guidance</a>]
    ├→ [<a href="#gumbel-max">Gumbel-Max</a> <a href="#sampling">샘플링</a>]
    ├→ [<a href="#confidence-based">신뢰도 계산</a>]
    └→ [<a href="#token-selection">토큰 선택</a>]
    ↓
[<a href="#post-processing">후처리</a>]
    ↓
[최종 출력]
                    </pre>
                </div>

                <h4>3.7 주요 파라미터의 영향</h4>
                <ul>
                    <li>temperature: <a href="#gumbel-max">Gumbel-Max</a> <a href="#sampling">샘플링</a>의 다양성 제어</li>
                    <li>cfg_scale: <a href="#classifier-free-guidance">Classifier-free Guidance</a>의 강도</li>
                    <li>block_length: <a href="#parallel">병렬 처리</a>의 효율성</li>
                    <li>steps: <a href="#denoising">디노이징</a> 과정의 세밀도</li>
                </ul>

                <h4>3.8 알고리즘 조합의 장점</h4>
                <ul>
                    <li>효율적인 <a href="#parallel">병렬 처리</a></li>
                    <li>높은 품질의 텍스트 생성</li>
                    <li>제어 가능한 생성 과정</li>
                    <li>수치적 안정성</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>III. Evaluation & Analysis</h2>
        <h3>4. Evaluation</h3>
        <div class="section-content">
            <h4>4.1 생성 방식 비교</h4>
            <div class="comparison-container">
                <div class="comparison-block">
                    <h5>기존 <a href="#llm">LLM</a> (<a href="#autoregressive">자동회귀</a> 방식)</h5>
                    <ul>
                        <li>순차적 토큰 생성
                            <ul>
                                <li>한 번에 하나의 토큰만 생성</li>
                                <li>이전 토큰에 의존적</li>
                                <li><a href="#parallel">병렬 처리</a> 불가능</li>
                            </ul>
                        </li>
                        <li>장점:
                            <ul>
                                <li>문맥의 일관성 유지</li>
                                <li>안정적인 생성 품질</li>
                                <li>검증된 기술</li>
                                <li>효율적인 키-값 캐싱</li>
                                <li>짧은 텍스트 생성에 효율적</li>
                            </ul>
                        </li>
                        <li>단점:
                            <ul>
                                <li>생성 속도가 느림</li>
                                <li><a href="#parallel">병렬화</a>가 어려움</li>
                                <li>긴 문장 생성 시 시간 소요</li>
                            </ul>
                        </li>
                    </ul>
                </div>

                <div class="comparison-block">
                    <h5><a href="#diffusion-llm">Diffusion LLM</a></h5>
                    <ul>
                        <li><a href="#parallel">병렬</a> 토큰 생성
                            <ul>
                                <li>블록 단위 동시 생성</li>
                                <li><a href="#masking">마스킹</a> 기반 예측</li>
                                <li>효율적인 <a href="#parallel">병렬 처리</a></li>
                            </ul>
                        </li>
                        <li>장점:
                            <ul>
                                <li>빠른 생성 속도 (최대 19배)</li>
                                <li>효율적인 <a href="#parallel">병렬 처리</a></li>
                                <li>유연한 생성 제어</li>
                                <li>높은 출력 다양성</li>
                                <li>긴 시퀀스 생성에 효율적</li>
                            </ul>
                        </li>
                        <li>단점:
                            <ul>
                                <li>문맥 일관성 유지의 어려움</li>
                                <li>메모리 사용량 증가</li>
                                <li>복잡한 구현</li>
                                <li>긴 입력 컨텍스트 처리 비효율</li>
                                <li>모든 <a href="#denoising">디노이징</a> 스텝 필요</li>
                            </ul>
                        </li>
                    </ul>
                </div>
            </div>

            <h4>4.2 성능 비교</h4>
            <div class="performance-comparison">
                <h5>4.2.1 생성 속도</h5>
                <ul>
                    <li><a href="#mercury-coder">MercuryCoder</a>:
                        <ul>
                            <li>NVIDIA H100에서 초당 ~1,000 토큰 생성</li>
                            <li>GPT-4 Mini 대비 19배 빠른 속도</li>
                            <li>코딩 벤치마크 성능 유지</li>
                        </ul>
                    </li>
                    <li><a href="#sedd">SEDD</a> (Score Entropy Discrete Diffusion):
                        <ul>
                            <li>GPT-2 대비 4.5배 빠른 추론</li>
                            <li>HellaSwag, WinoGrande 태스크에서 동등한 품질</li>
                        </ul>
                    </li>
                    <li><a href="#ar-diffusion">AR-DIFFUSION</a> (하이브리드):
                        <ul>
                            <li>요약 및 번역 태스크에서 최대 100배 빠른 추론</li>
                            <li>전통적 <a href="#autoregressive">자동회귀</a> <a href="#diffusion">디퓨전</a>과 비교 가능한 품질</li>
                        </ul>
                    </li>
                </ul>

                <h5>4.2.2 품질 및 다양성</h5>
                <ul>
                    <li><a href="#ssd-lm">SSD-LM</a>:
                        <ul>
                            <li>GPT-2와 동등하거나 더 나은 생성 품질</li>
                            <li>더 높은 다양성과 제어 가능성</li>
                        </ul>
                    </li>
                    <li><a href="#diffuseq">DiffuSeq</a>:
                        <ul>
                            <li>번역, 요약, 생성 태스크에서 강력한 성능</li>
                            <li>높은 출력 다양성</li>
                        </ul>
                    </li>
                    <li><a href="#ar-diffusion">AR-DIFFUSION</a>:
                        <ul>
                            <li>XSUM, CNN/DailyMail 요약 벤치마크에서 우수한 성능</li>
                            <li>IWSLT 번역에서 높은 BLEU 점수</li>
                        </ul>
                    </li>
                </ul>
            </div>

            <h4>4.3 확장성</h4>
            <div class="scalability-analysis">
                <h5>4.3.1 모델 크기</h5>
                <ul>
                    <li><a href="#autoregressive">자동회귀</a> <a href="#llm">LLM</a>: 
                        <ul>
                            <li>수조 개의 파라미터까지 확장 가능</li>
                            <li>효율적인 메모리 사용</li>
                        </ul>
                    </li>
                    <li><a href="#diffusion-llm">Diffusion LLM</a>:
                        <ul>
                            <li><a href="#lida">LLaDA</a>: 8B 파라미터로 <a href="#llama3">LLaMA3 8B</a>와 경쟁력 있는 성능</li>
                            <li><a href="#parallel">병렬 처리</a>로 인한 메모리 요구 증가</li>
                            <li>하이브리드 아키텍처로 확장성 개선</li>
                        </ul>
                    </li>
                </ul>

                <h5>4.3.2 학습 효율성</h5>
                <ul>
                    <li><a href="#autoregressive">자동회귀</a> <a href="#llm">LLM</a>: 
                        <ul>
                            <li>검증된 학습 방법론</li>
                            <li>안정적인 학습 과정</li>
                            <li>효율적인 컨텍스트 처리</li>
                        </ul>
                    </li>
                    <li><a href="#diffusion-llm">Diffusion LLM</a>:
                        <ul>
                            <li><a href="#masking">마스킹</a> 기반 학습으로 인한 효율성</li>
                            <li>복잡한 학습 과정</li>
                            <li>긴 컨텍스트 처리의 어려움</li>
                        </ul>
                    </li>
                </ul>
            </div>

            <h4>4.4 실용성</h4>
            <div class="practicality-analysis">
                <h5>4.4.1 리소스 요구사항</h5>
                <ul>
                    <li><a href="#autoregressive">자동회귀</a> <a href="#llm">LLM</a>:
                        <ul>
                            <li>상대적으로 적은 메모리 요구</li>
                            <li>순차적 처리로 인한 CPU 부하</li>
                            <li>효율적인 키-값 캐싱</li>
                        </ul>
                    </li>
                    <li><a href="#diffusion-llm">Diffusion LLM</a>:
                        <ul>
                            <li><a href="#parallel">병렬 처리</a>로 인한 높은 메모리 요구</li>
                            <li>GPU 활용 효율성</li>
                            <li>고성능 하드웨어 필요 (예: NVIDIA H100)</li>
                        </ul>
                    </li>
                </ul>

                <h5>4.4.2 적용 분야</h5>
                <ul>
                    <li><a href="#autoregressive">자동회귀</a> <a href="#llm">LLM</a>:
                        <ul>
                            <li>일반적인 대화 및 텍스트 생성</li>
                            <li>정확한 문맥 이해가 필요한 작업</li>
                            <li>짧은 텍스트 생성</li>
                            <li>복잡한 추론이 필요한 작업</li>
                        </ul>
                    </li>
                    <li><a href="#diffusion-llm">Diffusion LLM</a>:
                        <ul>
                            <li>빠른 응답이 필요한 실시간 서비스</li>
                            <li>대규모 텍스트 생성 작업</li>
                            <li>코드 생성 및 프로그래밍</li>
                            <li>다양한 출력이 필요한 창의적 작업</li>
                        </ul>
                    </li>
                </ul>
            </div>

            <h4>4.5 하이브리드 접근법</h4>
            <div class="hybrid-analysis">
                <p>
                    <a href="#ar-diffusion">AR-DIFFUSION</a>과 같은 하이브리드 아키텍처는 <a href="#autoregressive">자동회귀</a>와 <a href="#diffusion">디퓨전</a> 모델의 장점을 결합하여 새로운 가능성을 제시합니다:
                </p>
                <ul>
                    <li>속도와 품질의 균형
                        <ul>
                            <li>최대 100배 빠른 추론 속도</li>
                            <li><a href="#autoregressive">자동회귀</a> 모델과 비교 가능한 품질</li>
                        </ul>
                    </li>
                    <li>확장된 적용 분야
                        <ul>
                            <li>요약 및 번역 태스크에서 우수한 성능</li>
                            <li>높은 출력 다양성</li>
                        </ul>
                    </li>
                    <li>향후 발전 방향
                        <ul>
                            <li>효율적인 컨텍스트 처리 개선</li>
                            <li>메모리 사용량 최적화</li>
                            <li>하이브리드 아키텍처의 발전</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>IV. Related Work</h2>
        <div class="section-content">
            <ul>
                <li><a href="https://arxiv.org/abs/2502.09992" target="_blank">Large Language Diffusion Models (2025)</a> - Shen Nie et al.</li>
                <li><a href="https://arxiv.org/abs/1611.00712" target="_blank">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (2016)</a> - Maddison et al.</li>
                <li><a href="https://arxiv.org/abs/1611.01144" target="_blank">Categorical Reparameterization with Gumbel-Softmax (2016)</a> - Jang et al.</li>
                <li><a href="https://arxiv.org/abs/2112.10752" target="_blank">Diffusion-LM: Improving Controllable Text Generation by Sampling from Diffusion Models (2022)</a> - Li et al.</li>
                <li><a href="https://arxiv.org/abs/2307.01952" target="_blank">Stable Diffusion (2022)</a> - Rombach et al.</li>
                <li><a href="https://arxiv.org/abs/2205.14217" target="_blank">Scaling up Masked Diffusion Models on Text (2024)</a> - Nie et al.</li>
                <li><a href="https://www.inceptionlabs.ai/introducing-mercury" target="_blank">MercuryCoder (2024)</a> - Inception Labs</li>
                <li><a href="https://ml-gsai.github.io/LLaDA-demo/" target="_blank">LLaDA Demo (2025)</a> - Shen Nie et al.</li>
                <li><a href="https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct" target="_blank">LLaDA-8B-Instruct (2025)</a> - GSAI-ML</li>
            </ul>
        </div>
    </div>

    <div class="section">
        <h2>V. Conclusion</h2>
        <div class="section-content">
            <p>
                Diffusion LLM은 기존 자동회귀 방식의 LLM과는 다른 접근법을 제시합니다. 병렬 처리와 유연한 수정이 가능하다는 장점이 있지만, 복잡한 학습 과정과 높은 계산 비용이라는 단점도 있습니다. 특히 이산적 텍스트 생성 환경에서의 메모리 및 계산 효율화는 중요한 과제입니다.
            </p>
            <p>
                Stable Diffusion이 이미지 생성에서 사용하는 latent space와 같은 접근법이 텍스트 생성에도 적용될 수 있을 것으로 기대됩니다. 이를 통해 메모리 사용량을 줄이고 계산 효율성을 높일 수 있을 것입니다. 또한, 이러한 효율화 과정을 통해 더 빠른 텍스트 생성과 더 나은 품질의 결과를 얻을 수 있을 것입니다.
            </p>
            <p>
                하지만 텍스트 생성에서 latent space를 활용하는 것은 몇 가지 근본적인 어려움이 있습니다. 이미지는 연속적인 픽셀 값으로 표현되어 latent space에서의 보간이 자연스럽게 이루어질 수 있지만, 텍스트는 이산적인 토큰으로 구성되어 있어 latent space에서의 연속적인 변환이 어렵습니다. 또한, 텍스트의 의미적 구조와 문법적 규칙이 이미지보다 훨씬 복잡하고 엄격하기 때문에, latent space에서의 변환이 의미 있는 텍스트를 보장하기 어렵습니다. 이러한 특성으로 인해 텍스트 생성에서의 latent space 활용은 추가적인 연구와 새로운 접근 방식이 필요할 것으로 보입니다.
            </p>
            <p>
                향후 연구는 이러한 효율화 방안을 중심으로 진행되어야 하며, 특히 학습 효율성과 생성 품질의 개선에 초점을 맞추어야 할 것입니다. 또한, 근본적인 한계와 기술적 도전에 대한 명확한 이해를 바탕으로, 실용적인 응용 방안을 모색하는 것이 중요할 것입니다.
            </p>

            <h3>역할 분담</h3>
            <div class="member-roles">
                <p><strong>김도현</strong>: (공통)자료 조사 및 수집, 블로그 작성</p>
                <p><strong>이시웅</strong>: (공통)자료 조사 및 수집, 유튜브 녹화</p>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>VI. 주요 개념 설명</h2>
        <div class="concept-container">
            <div class="concept-item" id="transformer">
                <h3>Transformer</h3>
                <p>
                    Transformer는 2017년 Google이 제안한 딥러닝 아키텍처로, 자연어 처리 분야에서 혁신적인 성과를 이끌어낸 모델입니다.
                </p>
            </div>

            <div class="concept-item" id="diffusion-model">
                <h3>Stable Diffusion Model</h3>
                <p>
                    Stable Diffusion은 2022년 Stability AI에서 공개한 텍스트-이미지 생성 모델로, 
                    디퓨전 모델의 한 종류입니다.
                </p>
            </div>

            <div class="concept-item" id="diffusion">
                <h3>Diffusion</h3>
                <p>
                    디퓨전은 데이터에 점진적으로 노이즈를 추가했다가 제거하는 과정을 통해 
                    새로운 데이터를 생성하는 생성 모델의 한 종류입니다.
                </p>
            </div>

            <div class="concept-item" id="llada">
                <h3>LLaDA (Large Language Diffusion with mAsking)</h3>
                <p>8B 스케일의 디퓨전 기반 언어 모델로, 처음부터 완전히 학습되었으며 LLaMA3 8B와 비슷한 성능을 보여주는 혁신적인 모델입니다. 마스킹 기반의 디퓨전 방식을 사용하여 텍스트를 생성합니다.</p>
            </div>

            <div class="concept-item" id="mercury-coder">
                <h3>MercuryCoder</h3>
                <p>NVIDIA H100에서 초당 ~1,000 토큰을 생성할 수 있는 고성능 코드 생성 모델입니다. GPT-4 Mini 대비 19배 빠른 속도를 보여주며, 코딩 벤치마크에서도 우수한 성능을 유지합니다.</p>
            </div>

            <div class="concept-item" id="gemini-diffusion">
                <h3>Gemini Diffusion</h3>
                <p>Google DeepMind에서 개발한 디퓨전 기반 언어 모델로, 기존의 자동회귀적 모델과는 다른 방식으로 텍스트를 생성합니다.</p>
            </div>

            <div class="concept-item" id="llama3">
                <h3>LLaMA3 8B</h3>
                <p>Meta에서 개발한 8B 파라미터 규모의 자동회귀 기반 언어 모델로, LLaDA와 비교되는 기준 모델입니다.</p>
            </div>

            <div class="concept-item" id="diffusion-llm">
                <h3>디퓨전 LLM</h3>
                <p>기존의 자동회귀 방식이 아닌, 디퓨전 프로세스를 통해 텍스트를 생성하는 언어 모델입니다. 병렬 처리가 가능하고 빠른 생성 속도를 특징으로 합니다.</p>
            </div>

            <div class="concept-item" id="autoregressive">
                <h3>자동회귀 (Autoregressive)</h3>
                <p>이전 토큰에 기반하여 다음 토큰을 순차적으로 생성하는 방식입니다. GPT, LLaMA 등의 전통적인 언어 모델이 사용하는 방식입니다.</p>
            </div>

            <div class="concept-item" id="masking">
                <h3>마스킹 (Masking)</h3>
                <p>텍스트의 일부를 [MASK] 토큰으로 대체하여 모델이 해당 부분을 예측하도록 하는 기법입니다. BERT와 같은 모델에서 시작된 기술입니다.</p>
            </div>

            <div class="concept-item" id="parallel">
                <h3>병렬 처리 (Parallel Processing)</h3>
                <p>여러 작업을 동시에 처리하는 방식으로, 디퓨전 LLM에서는 여러 토큰을 동시에 생성할 수 있어 속도가 빠릅니다.</p>
            </div>

            <div class="concept-item" id="sampling">
                <h3>샘플링 (Sampling)</h3>
                <p>모델의 확률 분포에서 다음 토큰을 선택하는 과정입니다. Gumbel-Max, Gumbel-Softmax 등의 다양한 샘플링 기법이 사용됩니다.</p>
            </div>

            <div class="concept-item" id="denoising">
                <h3>디노이징 (Denoising)</h3>
                <p>노이즈가 있는 데이터에서 원래의 의미 있는 데이터를 복원하는 과정입니다. 디퓨전 모델의 핵심 과정입니다.</p>
            </div>

            <div class="concept-item" id="gumbel-max">
                <h3>Gumbel-Max 알고리즘</h3>
                <p>이산 확률 분포에서 샘플링을 수행하는 알고리즘으로, 수치적 안정성과 병렬 처리 가능성이 특징입니다.</p>
            </div>

            <div class="concept-item" id="classifier-free-guidance">
                <h3>Classifier-free Guidance</h3>
                <p>조건부 생성의 품질을 향상시키는 기법으로, 조건부/무조건부 생성의 가중 평균을 사용합니다.</p>
            </div>

            <div class="concept-item" id="block-parallel">
                <h3>블록 단위 반자동회귀</h3>
                <p>텍스트를 블록 단위로 나누어 병렬 처리하는 방식으로, 생성 속도를 크게 향상시킵니다.</p>
            </div>

            <div class="concept-item" id="confidence-based">
                <h3>신뢰도 기반 토큰 선택</h3>
                <p>모델의 예측 신뢰도를 기반으로 토큰을 선택하는 방식으로, 생성 품질을 향상시킵니다.</p>
            </div>

            <div class="concept-item" id="initialization">
                <h3>초기화 (Initialization)</h3>
                <p>텍스트 생성의 시작 단계로, 입력 텍스트를 토큰화하고 응답 부분을 [MASK] 토큰으로 초기화합니다.</p>
            </div>

            <div class="concept-item" id="block-processing">
                <h3>블록 처리 (Block Processing)</h3>
                <p>텍스트를 일정 크기의 블록으로 나누어 처리하는 단계입니다.</p>
            </div>

            <div class="concept-item" id="denoising-step">
                <h3>디노이징 단계 (Denoising Step)</h3>
                <p>노이즈가 있는 텍스트에서 의미 있는 텍스트로 변환하는 과정의 각 단계입니다.</p>
            </div>

            <div class="concept-item" id="post-processing">
                <h3>후처리 (Post-processing)</h3>
                <p>생성된 텍스트를 최종적으로 가공하는 단계입니다.</p>
            </div>

            <div class="concept-item" id="tokenization">
                <h3>토큰화 (Tokenization)</h3>
                <p>텍스트를 모델이 처리할 수 있는 토큰 단위로 분리하는 과정입니다.</p>
            </div>

            <div class="concept-item" id="mask">
                <h3>[MASK] 토큰</h3>
                <p>모델이 예측해야 할 부분을 나타내는 특수 토큰입니다.</p>
            </div>

            <div class="concept-item" id="remasking">
                <h3>리마스킹 (Remasking)</h3>
                <p>낮은 신뢰도를 가진 토큰을 다시 [MASK] 토큰으로 대체하는 과정입니다.</p>
            </div>

            <div class="concept-item" id="token-selection">
                <h3>토큰 선택 (Token Selection)</h3>
                <p>모델의 예측 결과에서 최종 토큰을 선택하는 과정입니다.</p>
            </div>

            <div class="concept-item" id="pretraining">
                <h3>사전학습 (Pretraining)</h3>
                <p>모델을 처음부터 학습시키는 단계로, 모든 토큰을 무작위로 마스킹하여 학습합니다.</p>
            </div>

            <div class="concept-item" id="sft">
                <h3>SFT (Supervised Fine-Tuning)</h3>
                <p>사전학습된 모델을 특정 태스크에 맞게 미세조정하는 과정입니다.</p>
            </div>
        </div>
    </div>

</body>
</html>

